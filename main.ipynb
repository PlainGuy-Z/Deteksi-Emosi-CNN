{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16458f8",
   "metadata": {},
   "source": [
    "**Blok 1: Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa365a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e69026",
   "metadata": {},
   "source": [
    "**Blok 2: Cek GPU dan Setup Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  current_device = torch.cuda.current_device()\n",
    "  gpu_properties = torch.cuda.get_device_properties(current_device)\n",
    "  print(f\"GPU Name: {gpu_properties.name}\")\n",
    "  print(f\"GPU Memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "  print(f\"GPU Compute Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "  print(\"No GPU available, using CPU\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e731a9b",
   "metadata": {},
   "source": [
    "**Blok 3: Setup Path dan Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "# Check if directories exist\n",
    "print(\"Checking dataset structure...\")\n",
    "if os.path.exists(train_dir) and os.path.exists(test_dir):\n",
    "  train_classes = os.listdir(train_dir)\n",
    "  test_classes = os.listdir(test_dir)\n",
    "  print(f\"Train classes: {train_classes}\")\n",
    "  print(f\"Test classes: {test_classes}\")\n",
    "    \n",
    "  # Count number of images per class\n",
    "  print(\"\\nNumber of images per class in train set:\")\n",
    "  for class_name in train_classes:\n",
    "    class_path = os.path.join(train_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "      num_images = len(os.listdir(class_path))\n",
    "      print(f\"  {class_name}: {num_images} images\")\n",
    "\n",
    "  print(\"\\nNumber of images per class in test set:\")\n",
    "  for class_name in test_classes:\n",
    "    class_path = os.path.join(test_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "      num_images = len(os.listdir(class_path))\n",
    "      print(f\"  {class_name}: {num_images} images\")\n",
    "else:\n",
    "  print(f\"Error: Dataset directories not found!\")\n",
    "  print(f\"Train directory: {train_dir}\")\n",
    "  print(f\"Test directory: {test_dir}\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74460d9",
   "metadata": {},
   "source": [
    "**Blok 4: Data Transforms dan Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data transformations with heavy augmentation\n",
    "train_transform = v2.Compose([\n",
    "  v2.Resize((224, 224)),  # Resize for ResNet\n",
    "  v2.RandomHorizontalFlip(p=0.5),\n",
    "  v2.RandomRotation(degrees=15),\n",
    "  v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "  v2.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "  v2.RandomGrayscale(p=0.1),\n",
    "  v2.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "  v2.ToImage(),\n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "  v2.Resize((224, 224)),\n",
    "  v2.ToImage(),\n",
    "  v2.ToDtype(torch.float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = val_transform  # Same as validation transform\n",
    "\n",
    "print(\"Data transforms defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c509c45",
   "metadata": {},
   "source": [
    "**Blok 5: Load Dataset dengan ImageFolder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets using ImageFolder\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "try:\n",
    "  train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "  test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "    \n",
    "  # Get class names\n",
    "  class_names = train_dataset.classes\n",
    "  print(f\"Class names: {class_names}\")\n",
    "  print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "  print(f\"Training samples: {len(train_dataset)}\")\n",
    "  print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "  # Split train into train and validation (90% train, 10% validation)\n",
    "  train_size = int(0.9 * len(train_dataset))\n",
    "  val_size = len(train_dataset) - train_size\n",
    "\n",
    "  train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    "  )\n",
    "    \n",
    "  # Apply validation transform to validation set\n",
    "  val_dataset.dataset = copy.deepcopy(val_dataset.dataset)\n",
    "  val_dataset.dataset.transform = val_transform\n",
    "\n",
    "  print(f\"After split - Training samples: {len(train_dataset)}\")\n",
    "  print(f\"After split - Validation samples: {len(val_dataset)}\")\n",
    "  print(f\"After split - Test samples: {len(test_dataset)}\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Error loading datasets: {e}\")\n",
    "  # Create dummy datasets for demonstration if real datasets not available\n",
    "  from torchvision.datasets import FakeData\n",
    "\n",
    "  train_dataset = FakeData(size=28000, image_size=(3, 48, 48), num_classes=7, \n",
    "                        transform=train_transform)\n",
    "  val_dataset = FakeData(size=3500, image_size=(3, 48, 48), num_classes=7, \n",
    "                        transform=val_transform)\n",
    "  test_dataset = FakeData(size=3500, image_size=(3, 48, 48), num_classes=7, \n",
    "                        transform=test_transform)\n",
    "  class_names = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835dd697",
   "metadata": {},
   "source": [
    "**Blok 6: Create DataLoaders dan Calculate Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with optimal settings\n",
    "train_loader = DataLoader(\n",
    "  train_dataset, \n",
    "  batch_size=batch_size, \n",
    "  shuffle=True, \n",
    "  num_workers=4,\n",
    "  pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "  val_dataset, \n",
    "  batch_size=batch_size, \n",
    "  shuffle=False, \n",
    "  num_workers=4,\n",
    "  pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "  test_dataset, \n",
    "  batch_size=batch_size, \n",
    "  shuffle=False, \n",
    "  num_workers=4,\n",
    "  pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "def calculate_class_weights(loader, num_classes=7):\n",
    "  class_counts = torch.zeros(num_classes)\n",
    "  for _, labels in loader:\n",
    "    for i in range(num_classes):\n",
    "      class_counts[i] += (labels == i).sum()\n",
    "    \n",
    "  total = class_counts.sum()\n",
    "  class_weights = total / (num_classes * class_counts)\n",
    "  return class_counts, class_weights\n",
    "\n",
    "class_counts, class_weights = calculate_class_weights(train_loader, num_classes)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "  print(f\"  {class_name}: {class_counts[i]:.0f} samples\")\n",
    "\n",
    "print(f\"\\nClass weights: {class_weights}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_names, class_counts.cpu().numpy())\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Emotion Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c0901",
   "metadata": {},
   "source": [
    "**Blok 7: Load Pretrained ResNet50 dan Modifikasi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a022e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionResNet50(nn.Module):\n",
    "  def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "    super(EmotionResNet50, self).__init__()\n",
    "        \n",
    "    # Load pretrained ResNet50\n",
    "    self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "    # Freeze early layers (first 4 blocks)\n",
    "    for name, param in self.backbone.named_parameters():\n",
    "      if 'layer1' in name or 'layer2' in name or 'layer3' in name:\n",
    "        param.requires_grad = False\n",
    "      else:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Replace the final fully connected layer\n",
    "    in_features = self.backbone.fc.in_features\n",
    "        \n",
    "    self.backbone.fc = nn.Sequential(\n",
    "      nn.Dropout(p=dropout_rate),\n",
    "      nn.Linear(in_features, 1024),\n",
    "      nn.BatchNorm1d(1024),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(p=dropout_rate-0.2),\n",
    "      nn.Linear(1024, 512),\n",
    "      nn.BatchNorm1d(512),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Dropout(p=dropout_rate-0.3),\n",
    "      nn.Linear(512, num_classes)\n",
    "    )\n",
    "        \n",
    "    # Initialize the new layers\n",
    "    self._initialize_weights(self.backbone.fc)\n",
    "    \n",
    "  def _initialize_weights(self, module):\n",
    "    for m in module.modules():\n",
    "      if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "          nn.init.constant_(m.bias, 0)\n",
    "      elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.backbone(x)\n",
    "\n",
    "# Initialize model\n",
    "try:\n",
    "  model = EmotionResNet50(num_classes=num_classes, dropout_rate=0.5)\n",
    "  model = model.to(device)\n",
    "  print(\"‚úÖ Model initialized successfully!\")\n",
    "  print(f\"Model architecture: ResNet50\")\n",
    "  print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "  # Count trainable parameters\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  print(f\"Total parameters: {total_params:,}\")\n",
    "  print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error initializing model: {e}\")\n",
    "  # Fallback: buat model sederhana jika ResNet50 gagal\n",
    "  print(\"Creating simple model as fallback...\")\n",
    "  model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, num_classes)\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35976a58",
   "metadata": {},
   "source": [
    "**Blok 8: Define Loss Function, Optimizer, dan Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a78d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with class weighting\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with different learning rates for different parts\n",
    "optimizer = optim.AdamW([\n",
    "  {'params': model.backbone.conv1.parameters(), 'lr': 1e-5},\n",
    "  {'params': model.backbone.bn1.parameters(), 'lr': 1e-5},\n",
    "  {'params': model.backbone.layer1.parameters(), 'lr': 1e-5},\n",
    "  {'params': model.backbone.layer2.parameters(), 'lr': 1e-5},\n",
    "  {'params': model.backbone.layer3.parameters(), 'lr': 1e-4},\n",
    "  {'params': model.backbone.layer4.parameters(), 'lr': 1e-4},\n",
    "  {'params': model.backbone.fc.parameters(), 'lr': 1e-3}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "  optimizer, \n",
    "  max_lr=[1e-5, 1e-5, 1e-5, 1e-5, 1e-4, 1e-4, 1e-3],\n",
    "  epochs=num_epochs,\n",
    "  steps_per_epoch=len(train_loader),\n",
    "  pct_start=0.1,\n",
    "  div_factor=10.0,\n",
    "  final_div_factor=100.0\n",
    ")\n",
    "\n",
    "print(\"Optimizer and scheduler defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbf431",
   "metadata": {},
   "source": [
    "**Blok 9: Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "  model.train()\n",
    "  running_loss = 0.0\n",
    "  correct_predictions = 0\n",
    "  total_samples = 0\n",
    "    \n",
    "  for batch_idx, (images, emotions) in enumerate(loader):\n",
    "    images, emotions = images.to(device), emotions.to(device)\n",
    "\n",
    "  # Monitor GPU memory setiap 50 batch\n",
    "    if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "      gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "      print(f'  GPU Memory: {gpu_memory:.2f} GB')\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, emotions)\n",
    "        \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "        \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "        \n",
    "    # Statistics\n",
    "    running_loss += loss.item() * images.size(0)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct_predictions += (predicted == emotions).sum().item()\n",
    "    total_samples += emotions.size(0)\n",
    "        \n",
    "    if batch_idx % 100 == 0:\n",
    "      current_lr = scheduler.get_last_lr()[0]\n",
    "      print(f'  Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}')\n",
    "    \n",
    "  epoch_loss = running_loss / total_samples\n",
    "  epoch_acc = correct_predictions / total_samples\n",
    "    \n",
    "  return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  correct_predictions = 0\n",
    "  total_samples = 0\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for images, emotions in loader:\n",
    "      images, emotions = images.to(device), emotions.to(device)\n",
    "            \n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, emotions)\n",
    "            \n",
    "      running_loss += loss.item() * images.size(0)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      correct_predictions += (predicted == emotions).sum().item()\n",
    "      total_samples += emotions.size(0)\n",
    "            \n",
    "      all_preds.extend(predicted.cpu().numpy())\n",
    "      all_labels.extend(emotions.cpu().numpy())\n",
    "    \n",
    "  epoch_loss = running_loss / total_samples\n",
    "  epoch_acc = correct_predictions / total_samples\n",
    "    \n",
    "  return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=50):\n",
    "  since = time.time()\n",
    "    \n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "  best_epoch = 0\n",
    "    \n",
    "  train_losses = []\n",
    "  train_accs = []\n",
    "  val_losses = []\n",
    "  val_accs = []\n",
    "    \n",
    "  for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    print('-' * 60)\n",
    "        \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "        \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "        \n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    print(f'Learning Rate: {current_lr:.2e}')\n",
    "        \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "      best_acc = val_acc\n",
    "      best_epoch = epoch\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      torch.save(model.state_dict(), 'best_emotion_model.pth')\n",
    "      print(f'*** New best model saved! Validation Accuracy: {val_acc:.4f} ***')\n",
    "        \n",
    "    print()\n",
    "    \n",
    "  time_elapsed = time.time() - since\n",
    "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "  print(f'Best Validation Accuracy: {best_acc:.4f} at epoch {best_epoch + 1}')\n",
    "    \n",
    "  # Load best model weights\n",
    "  model.load_state_dict(best_model_wts)\n",
    "    \n",
    "  return model, {\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a065e0",
   "metadata": {},
   "source": [
    "**Debugging Checklist (Jalankan blok ini sebelum blok 10)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b72f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING CHECKLIST - Jalankan blok ini sebelum Blok 10\n",
    "print(\"üîç DEBUGGING CHECKLIST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check 1: Apakah model ada?\n",
    "print(\"1. Model check:\")\n",
    "if 'model' in locals():\n",
    "  print(\"   ‚úÖ Model defined\")\n",
    "  print(f\"   Model type: {type(model)}\")\n",
    "  print(f\"   Model on device: {next(model.parameters()).device}\")\n",
    "else:\n",
    "  print(\"   ‚ùå Model NOT defined - Run Block 7 first!\")\n",
    "\n",
    "# Check 2: Apakah data loaders ada?\n",
    "print(\"\\n2. Data loaders check:\")\n",
    "loaders = ['train_loader', 'val_loader', 'test_loader']\n",
    "for loader in loaders:\n",
    "  if loader in locals():\n",
    "    print(f\"   ‚úÖ {loader} defined\")\n",
    "  else:\n",
    "    print(f\"   ‚ùå {loader} NOT defined\")\n",
    "\n",
    "# Check 3: Apakah optimizer dan criterion ada?\n",
    "print(\"\\n3. Training components check:\")\n",
    "components = ['criterion', 'optimizer', 'scheduler']\n",
    "for comp in components:\n",
    "  if comp in locals():\n",
    "    print(f\"   ‚úÖ {comp} defined\")\n",
    "  else:\n",
    "    print(f\"   ‚ùå {comp} NOT defined\")\n",
    "\n",
    "# Check 4: Apakah device tersedia?\n",
    "print(\"\\n4. Device check:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check 5: Test forward pass\n",
    "print(\"\\n5. Forward pass test:\")\n",
    "try:\n",
    "  if 'model' in locals() and 'train_loader' in locals():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, labels = sample_batch\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "      output = model(images)\n",
    "        \n",
    "      print(f\"   ‚úÖ Forward pass successful\")\n",
    "      print(f\"   Input shape: {images.shape}\")\n",
    "      print(f\"   Output shape: {output.shape}\")\n",
    "      print(f\"   Predicted classes: {torch.argmax(output, 1)}\")\n",
    "      print(f\"   Actual classes: {labels}\")\n",
    "except Exception as e:\n",
    "  print(f\"   ‚ùå Forward pass failed: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9ba71",
   "metadata": {},
   "source": [
    "**Blok 10: Training Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# DOUBLE CHECK GPU USAGE\n",
    "print(\"üîç Final GPU Verification:\")\n",
    "print(f\"Device being used: {device}\")\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")\n",
    "\n",
    "# Pastikan model benar-benar di GPU\n",
    "if next(model.parameters()).device.type != 'cuda':\n",
    "    print(\"‚ö†Ô∏è WARNING: Model is not on GPU! Moving to GPU...\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Model moved to: {next(model.parameters()).device}\")\n",
    "\n",
    "# Test GPU dengan forward pass kecil\n",
    "print(\"\\nüß™ Testing GPU with forward pass...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Ambil batch kecil untuk test\n",
    "        test_batch, test_labels = next(iter(train_loader))\n",
    "        test_batch = test_batch.to(device)\n",
    "        test_output = model(test_batch)\n",
    "        print(f\"‚úÖ GPU test successful!\")\n",
    "        print(f\"   Input batch shape: {test_batch.shape}\")\n",
    "        print(f\"   Output shape: {test_output.shape}\")\n",
    "        print(f\"   Batch device: {test_batch.device}\")\n",
    "        print(f\"   Output device: {test_output.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GPU test failed: {e}\")\n",
    "\n",
    "# Clear GPU cache sebelum training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üßπ GPU cache cleared\")\n",
    "\n",
    "print(\"\\nüöÄ STARTING TRAINING WITH RESNET50...\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# TRAINING EXECUTION\n",
    "try:\n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, \n",
    "        optimizer, scheduler, device, num_epochs\n",
    "    )\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    \n",
    "    # Tampilkan best accuracy\n",
    "    best_val_acc = max(history['val_accs'])\n",
    "    print(f\"üéâ Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"‚ùå GPU Out of Memory! Trying solutions...\")\n",
    "        # Solusi OOM: kurangi batch size\n",
    "        print(\"Reducing batch size to 32...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Coba training lagi dengan batch size lebih kecil\n",
    "        model, history = train_model(\n",
    "            model, train_loader, val_loader, criterion, \n",
    "            optimizer, scheduler, device, num_epochs\n",
    "        )\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # Fallback: training sederhana\n",
    "    print(\"Using simplified training...\")\n",
    "    simple_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    simple_scheduler = lr_scheduler.StepLR(simple_optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    model, history = train_model(\n",
    "        model, train_loader, val_loader, criterion, \n",
    "        simple_optimizer, simple_scheduler, device, num_epochs=30\n",
    "    )\n",
    "\n",
    "print(\"Training process completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
